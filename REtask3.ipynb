{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Helper notebook for loading the data and saving the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "from mvseg.mvseg.utils.experiments import load_experiment\n",
    "import glob\n",
    "import torch\n",
    "from mvseg.mvseg.datasets import get_dataset\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import signal\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from mvseg.mvseg.datasets import get_dataset\n",
    "from mvseg.settings import TRAINING_PATH\n",
    "from mvseg import logger\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n",
    "def save_zipped_pickle(obj, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, make predictions and save prediction in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = load_zipped_pickle(\"/cluster/project/infk/cvg/students/alpaul/MitralValveSegmentation/data/train.pkl\")\n",
    "test_data = load_zipped_pickle(\"/cluster/project/infk/cvg/students/alpaul/MitralValveSegmentation/data/test.pkl\")\n",
    "# samples = load_zipped_pickle(\"sample.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get basic dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data info:\n",
      "    Number of train videos: 65\n",
      "    Min num frames in train videos: 54\n",
      "    Max num frames in train videos: 334\n",
      "    Average num of frames in train videos: 151.83076923076922\n",
      "Test data info:\n",
      "    Number of test videos: 20\n",
      "    Min num frames in test videos: 39\n",
      "    Max num frames in test videos: 125\n",
      "    Average num of frames in test videos: 75.35\n",
      "Shapes: [(586, 821, 103), (587, 791, 52), (583, 777, 69), (582, 851, 61), (732, 845, 53), (583, 809, 84), (582, 737, 78), (587, 775, 125), (730, 956, 76), (587, 781, 104), (583, 681, 68), (587, 713, 90), (587, 612, 78), (587, 773, 73), (707, 855, 39), (731, 1007, 72), (583, 780, 106), (583, 670, 63), (594, 745, 51), (583, 779, 62)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data info:\")\n",
    "print(f\"    Number of train videos: {np.array(train_data).shape[0]}\")\n",
    "print(f\"    Min num frames in train videos: {min([data['video'].shape[2] for data in train_data])}\")\n",
    "print(f\"    Max num frames in train videos: {max([data['video'].shape[2] for data in train_data])}\")\n",
    "print(f\"    Average num of frames in train videos: {np.mean([data['video'].shape[2] for data in train_data])}\")\n",
    "# print([np.array(train_data[i]['box']).shape for i in range(len(train_data))])\n",
    "\n",
    "print(f\"Test data info:\")\n",
    "print(f\"    Number of test videos: {np.array(test_data).shape[0]}\")\n",
    "print(f\"    Min num frames in test videos: {min([data['video'].shape[2] for data in test_data])}\")\n",
    "print(f\"    Max num frames in test videos: {max([data['video'].shape[2] for data in test_data])}\")\n",
    "print(f\"    Average num of frames in test videos: {np.mean([data['video'].shape[2] for data in test_data])}\")\n",
    "print(f\"Shapes: {[data['video'].shape for data in test_data]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create videos for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_videos_to_save = 65\n",
    "\n",
    "train = deepcopy(train_data)\n",
    "for i in range(num_videos_to_save):\n",
    "    video = train[i]['video']\n",
    "    box = np.array(train[i]['box'])\n",
    "    label = train[i]['label']\n",
    "    label = label.astype('float32')\n",
    "#     print(label)\n",
    "#     label = label*255\n",
    "#     print(list(label[train[i]['frames'][0]]))\n",
    "    label = np.moveaxis(label, -1, 0)\n",
    "    video = np.moveaxis(video, -1, 0)\n",
    "    first_image = video[0]\n",
    "    size = first_image.shape\n",
    "    fps = 10\n",
    "    out = cv2.VideoWriter(f'/cluster/project/infk/cvg/students/alpaul/MitralValveSegmentation/data/train_videos/{i}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (size[1], size[0]), False)\n",
    "    for j in range(len(video)):\n",
    "        image = video[j]\n",
    "#         image[box] = 255\n",
    "        labelj = label[j]\n",
    "        image[labelj] = 255\n",
    "        box = box.astype('uint8')*255\n",
    "        label = label.astype('uint8')*255\n",
    "        out.write(image)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate videos or submission array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12/18/2022 12:22:58 mvseg INFO] Starting test exp2_stage2_ft\n",
      "[12/18/2022 12:22:58 mvseg.mvseg.datasets.base_dataset INFO] Creating dataset MVSegDataset\n",
      "/cluster/home/alpaul/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "[12/18/2022 12:23:18 mvseg.mvseg.utils.experiments INFO] Loading checkpoint checkpoint_best.tar\n"
     ]
    }
   ],
   "source": [
    "experiment = 'exp2_stage2_ft'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "conf = '/cluster/home/alpaul/MitralValveSegmentation/mvseg/mvseg/configs/config_train.yaml'\n",
    "logger.info(f'Starting test {experiment}')\n",
    "output_dir = Path(TRAINING_PATH, experiment)\n",
    "conf = OmegaConf.merge(OmegaConf.load(conf), {'train':{'num_workers': 0}})\n",
    "data_conf = conf.data\n",
    "dataset = get_dataset(data_conf.name)(data_conf)\n",
    "test_loader = dataset.get_data_loader('test')\n",
    "model = load_experiment(experiment, conf.model)\n",
    "model = model.eval()\n",
    "loss_fn, metrics_fn = model.loss, model.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through dataloader and write video. Optionally, store all predicted masks in array for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "threshold = 0.3\n",
    "all_preds = {}\n",
    "all_images = {}\n",
    "fps = 10\n",
    "\n",
    "submit = False # Set false if you want to generate video instead\n",
    "\n",
    "if not submit:\n",
    "    out = cv2.VideoWriter(f'{experiment}_test_full_t{threshold}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (224, 224), False) # compatible 821, 586\n",
    "\n",
    "for data in tqdm(test_loader, desc='Testing', ascii=True, disable=False):\n",
    "    count += 1\n",
    "    with torch.no_grad():   \n",
    "        pred = model(data)\n",
    "        mask_pred = pred['seg'].squeeze(0).squeeze(0)\n",
    "        h,w = data['hw']\n",
    "        if submit:\n",
    "            mask_pred = torch.tensor(resize(mask_pred, (h[0],w[0]), anti_aliasing=True))\n",
    "        mask_pred = torch.gt(mask_pred, threshold)\n",
    "        mask_pred = mask_pred.numpy().astype('bool')\n",
    "        if not submit:\n",
    "            im = data['image'].squeeze(0).squeeze(0).numpy()\n",
    "#             im = resize(im, (h[0],w[0]), anti_aliasing=True)\n",
    "            assert im.shape == mask_pred.shape\n",
    "            im = im * 255\n",
    "            im[mask_pred] = 255\n",
    "            im = im.astype('uint8')       \n",
    "            out.write(im)\n",
    "        if submit:\n",
    "            if data['video'][0] not in all_preds.keys():\n",
    "                all_preds[data['video'][0]] = [mask_pred]\n",
    "            else:\n",
    "                all_preds[data['video'][0]].append(mask_pred)\n",
    "\n",
    "if not submit:\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for d in test_data:\n",
    "    prediction = np.array([list(p) for p in all_preds[d['name']]])\n",
    "    prediction = np.moveaxis(prediction, 0, -1)\n",
    "    assert prediction.shape == d['video'].shape\n",
    "    predictions.append({\n",
    "        'name': d['name'],\n",
    "        'prediction': prediction\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in correct format\n",
    "save_zipped_pickle(predictions, f'my_predictions_{threshold}_retrain.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore this block: Loop through test dataset, and generate 1 video per test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output_directory = '/cluster/home/alpaul/videos'\n",
    "# threshold = 0.7\n",
    "# fps = 10\n",
    "# predictions = []\n",
    "\n",
    "# for d in tqdm(test_data):\n",
    "\n",
    "#     d_video_frames = np.moveaxis(d['video'], -1, 0)\n",
    "#     print(d_video_frames.shape)\n",
    "#     height, width = d_video_frames.shape[1], d_video_frames.shape[2]\n",
    "#     out = cv2.VideoWriter(f'{output_directory}/{d[\"name\"]}_{threshold}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height), False)\n",
    "#     prediction = []\n",
    "#     count = 0\n",
    "#     with torch.no_grad():\n",
    "#         for idx in tqdm(range(len(d_video_frames))):\n",
    "#             count += 1\n",
    "#             d_im = list(filter(lambda p : p['frame_number'] == idx and p['video'][0] == d['name'], test_loader))[0]\n",
    "#             pred = model(d_im)\n",
    "#             mask_pred = pred['seg'].squeeze(0).squeeze(0)\n",
    "#             h,w = d_im['hw'] # height to resize to\n",
    "#             mask_pred = torch.tensor(resize(mask_pred, (h[0],w[0]), anti_aliasing=True))\n",
    "#             mask_pred = torch.gt(mask_pred, threshold)\n",
    "#             mask_pred = mask_pred.numpy().astype('bool')\n",
    "#             im = d_im['image'].squeeze(0).squeeze(0)\n",
    "#             im = resize(im, (h[0],w[0]), anti_aliasing=True)\n",
    "#             assert im.shape == mask_pred.shape\n",
    "#             im = im * 255\n",
    "#             im[mask_pred] = 255\n",
    "#             im = im.astype('uint8')\n",
    "#             prediction.append(mask_pred)\n",
    "#             out.write(im.astype('uint8'))\n",
    "# #             plt.imshow(mask_pred.astype('float32')*255)\n",
    "# #             print(np.max(mask_pred.astype('uint8')*255))\n",
    "#     out.release()\n",
    "#     del out\n",
    "#     print(f\"shape of prediction list: {np.array(prediction).shape}\")\n",
    "#     prediction = np.moveaxis(prediction, 0, -1)\n",
    "#     print(f\"shape of prediction list: {prediction.shape}\")\n",
    "#     predictions.append({\n",
    "#         'name': d['name'],\n",
    "#         'prediction': prediction\n",
    "#         }\n",
    "#     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
